{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecf7b7a7",
   "metadata": {},
   "source": [
    "# Natural Language Toolkit\n",
    "\n",
    "Für die Generierung von Tokens aus Text gibt es bereits eine Library in Python, die wir verwenden können.\n",
    "Sie heißt `nltk`, siehe <https://nltk.org>.\n",
    "\n",
    "Sie ist um einiges umfangreicher und kann nicht nur Tokens und Worthäufigkeiten aus einem Text erstellen,\n",
    "sondern auch Texte semantisch analysieren.\n",
    "\n",
    "Damit nltk für die semantische Analyse von Texten eingesetzt werden kann, müssen Sprach-spezifische Daten heruntergeladen und über die python3-REPL konfiguriert werden via\n",
    "\n",
    "```py\n",
    ">>> import nltk\n",
    ">>> nltk.download() # interaktive shell zum downloaden von Corpora.\n",
    "```\n",
    "\n",
    "## Deutsch?\n",
    "\n",
    "Deutsch ist da nicht im Downloadkatalog, aber die Uni Tübingen hat GermaNet entwickelt; allerdings nur für nichtkommerzielle Zwecke. \n",
    "Weitere Informationen zu GermaNet gibt es hier: <https://uni-tuebingen.de/fakultaeten/philosophische-fakultaet/fachbereiche/neuphilologie/seminar-fuer-sprachwissenschaft/arbeitsbereiche/allg-sprachwissenschaft-computerlinguistik/ressourcen/lexica/germanet-1/>\n",
    "\n",
    "Zusätzlich hat dann die Uni Stuttgart ein GermanNLTK entwickelt. Da beides ausschließlich für akademische Zwecke kostenfrei verfügbar ist,\n",
    "arbeiten wir erstmal mit der englischen Sprache.\n",
    "\n",
    "## Englisch\n",
    "\n",
    "Lass uns das Toolkit mal mit der englischen Sprache ausprobieren, am Beispiel der Rede von Martin Luther King.\n",
    "Hierfür laden wir ein paar Resourcen herunter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c44ffb75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package english_wordnet to\n",
      "[nltk_data]     /Users/lea.rosema/nltk_data...\n",
      "[nltk_data]   Package english_wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/lea.rosema/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
      "[nltk_data]     /Users/lea.rosema/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker_tab is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/lea.rosema/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /Users/lea.rosema/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('english_wordnet')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('maxent_ne_chunker_tab')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "346f96ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'have', 'a', 'dream', 'that', 'my', 'four', 'little', 'children', 'will', 'one', 'day', 'live', 'in', 'a', 'nation', 'where', 'they', 'will', 'not', 'be', 'judged', 'by', 'the', 'color', 'of', 'their', 'skin', 'but', 'by', 'the', 'content', 'of', 'their', 'character', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "speech = \"\"\"\n",
    "I have a dream that my four little children\n",
    "will one day live in a nation where they will\n",
    "not be judged by the color of their skin but\n",
    "by the content of their character.\n",
    "\"\"\"\n",
    "\n",
    "tokens = word_tokenize(speech)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5061a4",
   "metadata": {},
   "source": [
    "Desweiteren kann nltk die Tokens taggen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ebae0d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 'PRP'), ('have', 'VBP'), ('a', 'DT'), ('dream', 'NN'), ('that', 'IN'), ('my', 'PRP$'), ('four', 'CD'), ('little', 'JJ'), ('children', 'NNS'), ('will', 'MD'), ('one', 'CD'), ('day', 'NN'), ('live', 'VB'), ('in', 'IN'), ('a', 'DT'), ('nation', 'NN'), ('where', 'WRB'), ('they', 'PRP'), ('will', 'MD'), ('not', 'RB'), ('be', 'VB'), ('judged', 'VBN'), ('by', 'IN'), ('the', 'DT'), ('color', 'NN'), ('of', 'IN'), ('their', 'PRP$'), ('skin', 'NN'), ('but', 'CC'), ('by', 'IN'), ('the', 'DT'), ('content', 'NN'), ('of', 'IN'), ('their', 'PRP$'), ('character', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "tagged_tokens = nltk.pos_tag(tokens)\n",
    "\n",
    "print(tagged_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a454c3ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  I/PRP\n",
      "  have/VBP\n",
      "  a/DT\n",
      "  dream/NN\n",
      "  that/IN\n",
      "  my/PRP$\n",
      "  four/CD\n",
      "  little/JJ\n",
      "  children/NNS\n",
      "  will/MD\n",
      "  one/CD\n",
      "  day/NN\n",
      "  live/VB\n",
      "  in/IN\n",
      "  a/DT\n",
      "  nation/NN\n",
      "  where/WRB\n",
      "  they/PRP\n",
      "  will/MD\n",
      "  not/RB\n",
      "  be/VB\n",
      "  judged/VBN\n",
      "  by/IN\n",
      "  the/DT\n",
      "  color/NN\n",
      "  of/IN\n",
      "  their/PRP$\n",
      "  skin/NN\n",
      "  but/CC\n",
      "  by/IN\n",
      "  the/DT\n",
      "  content/NN\n",
      "  of/IN\n",
      "  their/PRP$\n",
      "  character/NN\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "entities = nltk.chunk.ne_chunk(tagged_tokens)\n",
    "\n",
    "print(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c7aecb34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" baseProfile=\"full\" height=\"120px\" preserveAspectRatio=\"xMidYMid meet\" style=\"font-family: times, serif; font-weight: normal; font-style: normal; font-size: 16px\" version=\"1.1\" viewBox=\"0,0,1704.0,120.0\" width=\"1704px\"><defs/><svg width=\"100%\" x=\"0\" y=\"0px\"><defs/><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">S</text></svg><svg width=\"2.34742%\" x=\"0%\" y=\"48px\"><defs/><svg width=\"100%\" x=\"0\" y=\"0px\"><defs/><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">I</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs/><svg width=\"100%\" x=\"0\" y=\"0px\"><defs/><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">PRP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\"/></svg><line stroke=\"black\" x1=\"50%\" x2=\"1.17371%\" y1=\"20px\" y2=\"48px\"/><svg width=\"2.8169%\" x=\"2.34742%\" y=\"48px\"><defs/><svg width=\"100%\" x=\"0\" y=\"0px\"><defs/><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">have</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs/><svg width=\"100%\" x=\"0\" y=\"0px\"><defs/><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VBP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\"/></svg><line stroke=\"black\" x1=\"50%\" x2=\"3.75587%\" y1=\"20px\" y2=\"48px\"/><svg width=\"1.87793%\" x=\"5.16432%\" y=\"48px\"><defs/><svg width=\"100%\" x=\"0\" y=\"0px\"><defs/><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">a</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs/><svg width=\"100%\" x=\"0\" y=\"0px\"><defs/><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">DT</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\"/></svg><line stroke=\"black\" x1=\"50%\" x2=\"6.10329%\" y1=\"20px\" y2=\"48px\"/><svg width=\"3.28638%\" x=\"7.04225%\" y=\"48px\"><defs/><svg width=\"100%\" x=\"0\" y=\"0px\"><defs/><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">dream</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs/><svg width=\"100%\" x=\"0\" y=\"0px\"><defs/><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\"/></svg><line stroke=\"black\" x1=\"50%\" x2=\"8.68545%\" y1=\"20px\" y2=\"48px\"/><svg width=\"2.8169%\" x=\"10.3286%\" y=\"48px\"><defs/><svg width=\"100%\" x=\"0\" y=\"0px\"><defs/><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">that</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs/><svg width=\"100%\" x=\"0\" y=\"0px\"><defs/><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">IN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\"/></svg><line stroke=\"black\" x1=\"50%\" x2=\"11.7371%\" y1=\"20px\" y2=\"48px\"/><svg width=\"2.8169%\" x=\"13.1455%\" y=\"48px\"><defs/><svg width=\"100%\" x=\"0\" y=\"0px\"><defs/><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">my</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs/><svg width=\"100%\" x=\"0\" y=\"0px\"><defs/><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">PRP$</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\"/></svg><line stroke=\"black\" x1=\"50%\" x2=\"14.554%\" y1=\"20px\" y2=\"48px\"/><svg width=\"2.8169%\" x=\"15.9624%\" y=\"48px\"><defs/><svg width=\"100%\" x=\"0\" y=\"0px\"><defs/><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">four</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs/><svg width=\"100%\" x=\"0\" y=\"0px\"><defs/><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">CD</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\"/></svg><line stroke=\"black\" x1=\"50%\" x2=\"17.3709%\" y1=\"20px\" y2=\"48px\"/><svg width=\"3.75587%\" x=\"18.7793%\" y=\"48px\"><defs/><svg width=\"100%\" x=\"0\" y=\"0px\"><defs/><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">little</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs/><svg width=\"100%\" x=\"0\" y=\"0px\"><defs/><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">JJ</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\"/></svg><line stroke=\"black\" x1=\"50%\" x2=\"20.6573%\" y1=\"20px\" y2=\"48px\"/><svg width=\"4.69484%\" x=\"22.5352%\" y=\"48px\"><defs/><svg width=\"100%\" x=\"0\" y=\"0px\"><defs/><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">children</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs/><svg width=\"100%\" x=\"0\" y=\"0px\"><defs/><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNS</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\"/></svg><line stroke=\"black\" x1=\"50%\" x2=\"24.8826%\" y1=\"20px\" y2=\"48px\"/><svg width=\"2.8169%\" x=\"27.23%\" y=\"48px\"><defs/><svg width=\"100%\" x=\"0\" y=\"0px\"><defs/><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">will</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs/><svg width=\"100%\" x=\"0\" y=\"0px\"><defs/><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">MD</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\"/></svg><line stroke=\"black\" x1=\"50%\" x2=\"28.6385%\" y1=\"20px\" y2=\"48px\"/><svg width=\"2.34742%\" x=\"30.0469%\" y=\"48px\"><defs/><svg width=\"100%\" x=\"0\" y=\"0px\"><defs/><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">one</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs/><svg width=\"100%\" x=\"0\" y=\"0px\"><defs/><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">CD</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\"/></svg><line stroke=\"black\" x1=\"50%\" x2=\"31.2207%\" y1=\"20px\" y2=\"48px\"/><svg width=\"2.34742%\" x=\"32.3944%\" y=\"48px\"><defs/><svg width=\"100%\" x=\"0\" y=\"0px\"><defs/><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">day</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs/><svg width=\"100%\" x=\"0\" y=\"0px\"><defs/><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\"/></svg><line stroke=\"black\" x1=\"50%\" x2=\"33.5681%\" y1=\"20px\" y2=\"48px\"/><svg width=\"2.8169%\" x=\"34.7418%\" y=\"48px\"><defs/><svg width=\"100%\" x=\"0\" y=\"0px\"><defs/><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">live</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs/><svg width=\"100%\" x=\"0\" y=\"0px\"><defs/><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VB</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\"/></svg><line stroke=\"black\" x1=\"50%\" x2=\"36.1502%\" y1=\"20px\" y2=\"48px\"/><svg width=\"1.87793%\" x=\"37.5587%\" y=\"48px\"><defs/><svg width=\"100%\" x=\"0\" y=\"0px\"><defs/><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">in</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs/><svg width=\"100%\" x=\"0\" y=\"0px\"><defs/><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">IN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\"/></svg><line stroke=\"black\" x1=\"50%\" x2=\"38.4977%\" y1=\"20px\" y2=\"48px\"/><svg width=\"1.87793%\" x=\"39.4366%\" y=\"48px\"><defs/><svg width=\"100%\" x=\"0\" y=\"0px\"><defs/><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">a</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs/><svg width=\"100%\" x=\"0\" y=\"0px\"><defs/><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">DT</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\"/></svg><line stroke=\"black\" x1=\"50%\" x2=\"40.3756%\" y1=\"20px\" y2=\"48px\"/><svg width=\"3.75587%\" x=\"41.3146%\" y=\"48px\"><defs/><svg width=\"100%\" x=\"0\" y=\"0px\"><defs/><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">nation</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs/><svg width=\"100%\" x=\"0\" y=\"0px\"><defs/><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\"/></svg><line stroke=\"black\" x1=\"50%\" x2=\"43.1925%\" y1=\"20px\" y2=\"48px\"/><svg width=\"3.28638%\" x=\"45.0704%\" y=\"48px\"><defs/><svg width=\"100%\" x=\"0\" y=\"0px\"><defs/><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">where</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs/><svg width=\"100%\" x=\"0\" y=\"0px\"><defs/><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">WRB</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\"/></svg><line stroke=\"black\" x1=\"50%\" x2=\"46.7136%\" y1=\"20px\" y2=\"48px\"/><svg width=\"2.8169%\" x=\"48.3568%\" y=\"48px\"><defs/><svg width=\"100%\" x=\"0\" y=\"0px\"><defs/><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">they</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs/><svg width=\"100%\" x=\"0\" y=\"0px\"><defs/><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">PRP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\"/></svg><line stroke=\"black\" x1=\"50%\" x2=\"49.7653%\" y1=\"20px\" y2=\"48px\"/><svg width=\"2.8169%\" x=\"51.1737%\" y=\"48px\"><defs/><svg width=\"100%\" x=\"0\" y=\"0px\"><defs/><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">will</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs/><svg width=\"100%\" x=\"0\" y=\"0px\"><defs/><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">MD</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\"/></svg><line stroke=\"black\" x1=\"50%\" x2=\"52.5822%\" y1=\"20px\" y2=\"48px\"/><svg width=\"2.34742%\" x=\"53.9906%\" y=\"48px\"><defs/><svg width=\"100%\" x=\"0\" y=\"0px\"><defs/><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">not</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs/><svg width=\"100%\" x=\"0\" y=\"0px\"><defs/><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">RB</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\"/></svg><line stroke=\"black\" x1=\"50%\" x2=\"55.1643%\" y1=\"20px\" y2=\"48px\"/><svg width=\"1.87793%\" x=\"56.338%\" y=\"48px\"><defs/><svg width=\"100%\" x=\"0\" y=\"0px\"><defs/><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">be</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs/><svg width=\"100%\" x=\"0\" y=\"0px\"><defs/><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VB</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\"/></svg><line stroke=\"black\" x1=\"50%\" x2=\"57.277%\" y1=\"20px\" y2=\"48px\"/><svg width=\"3.75587%\" x=\"58.216%\" y=\"48px\"><defs/><svg width=\"100%\" x=\"0\" y=\"0px\"><defs/><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">judged</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs/><svg width=\"100%\" x=\"0\" y=\"0px\"><defs/><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VBN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\"/></svg><line stroke=\"black\" x1=\"50%\" x2=\"60.0939%\" y1=\"20px\" y2=\"48px\"/><svg width=\"1.87793%\" x=\"61.9718%\" y=\"48px\"><defs/><svg width=\"100%\" x=\"0\" y=\"0px\"><defs/><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">by</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs/><svg width=\"100%\" x=\"0\" y=\"0px\"><defs/><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">IN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\"/></svg><line stroke=\"black\" x1=\"50%\" x2=\"62.9108%\" y1=\"20px\" y2=\"48px\"/><svg width=\"2.34742%\" x=\"63.8498%\" y=\"48px\"><defs/><svg width=\"100%\" x=\"0\" y=\"0px\"><defs/><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">the</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs/><svg width=\"100%\" x=\"0\" y=\"0px\"><defs/><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">DT</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\"/></svg><line stroke=\"black\" x1=\"50%\" x2=\"65.0235%\" y1=\"20px\" y2=\"48px\"/><svg width=\"3.28638%\" x=\"66.1972%\" y=\"48px\"><defs/><svg width=\"100%\" x=\"0\" y=\"0px\"><defs/><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">color</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs/><svg width=\"100%\" x=\"0\" y=\"0px\"><defs/><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\"/></svg><line stroke=\"black\" x1=\"50%\" x2=\"67.8404%\" y1=\"20px\" y2=\"48px\"/><svg width=\"1.87793%\" x=\"69.4836%\" y=\"48px\"><defs/><svg width=\"100%\" x=\"0\" y=\"0px\"><defs/><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">of</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs/><svg width=\"100%\" x=\"0\" y=\"0px\"><defs/><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">IN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\"/></svg><line stroke=\"black\" x1=\"50%\" x2=\"70.4225%\" y1=\"20px\" y2=\"48px\"/><svg width=\"3.28638%\" x=\"71.3615%\" y=\"48px\"><defs/><svg width=\"100%\" x=\"0\" y=\"0px\"><defs/><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">their</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs/><svg width=\"100%\" x=\"0\" y=\"0px\"><defs/><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">PRP$</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\"/></svg><line stroke=\"black\" x1=\"50%\" x2=\"73.0047%\" y1=\"20px\" y2=\"48px\"/><svg width=\"2.8169%\" x=\"74.6479%\" y=\"48px\"><defs/><svg width=\"100%\" x=\"0\" y=\"0px\"><defs/><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">skin</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs/><svg width=\"100%\" x=\"0\" y=\"0px\"><defs/><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\"/></svg><line stroke=\"black\" x1=\"50%\" x2=\"76.0563%\" y1=\"20px\" y2=\"48px\"/><svg width=\"2.34742%\" x=\"77.4648%\" y=\"48px\"><defs/><svg width=\"100%\" x=\"0\" y=\"0px\"><defs/><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">but</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs/><svg width=\"100%\" x=\"0\" y=\"0px\"><defs/><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">CC</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\"/></svg><line stroke=\"black\" x1=\"50%\" x2=\"78.6385%\" y1=\"20px\" y2=\"48px\"/><svg width=\"1.87793%\" x=\"79.8122%\" y=\"48px\"><defs/><svg width=\"100%\" x=\"0\" y=\"0px\"><defs/><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">by</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs/><svg width=\"100%\" x=\"0\" y=\"0px\"><defs/><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">IN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\"/></svg><line stroke=\"black\" x1=\"50%\" x2=\"80.7512%\" y1=\"20px\" y2=\"48px\"/><svg width=\"2.34742%\" x=\"81.6901%\" y=\"48px\"><defs/><svg width=\"100%\" x=\"0\" y=\"0px\"><defs/><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">the</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs/><svg width=\"100%\" x=\"0\" y=\"0px\"><defs/><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">DT</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\"/></svg><line stroke=\"black\" x1=\"50%\" x2=\"82.8638%\" y1=\"20px\" y2=\"48px\"/><svg width=\"4.22535%\" x=\"84.0376%\" y=\"48px\"><defs/><svg width=\"100%\" x=\"0\" y=\"0px\"><defs/><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">content</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs/><svg width=\"100%\" x=\"0\" y=\"0px\"><defs/><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\"/></svg><line stroke=\"black\" x1=\"50%\" x2=\"86.1502%\" y1=\"20px\" y2=\"48px\"/><svg width=\"1.87793%\" x=\"88.2629%\" y=\"48px\"><defs/><svg width=\"100%\" x=\"0\" y=\"0px\"><defs/><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">of</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs/><svg width=\"100%\" x=\"0\" y=\"0px\"><defs/><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">IN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\"/></svg><line stroke=\"black\" x1=\"50%\" x2=\"89.2019%\" y1=\"20px\" y2=\"48px\"/><svg width=\"3.28638%\" x=\"90.1408%\" y=\"48px\"><defs/><svg width=\"100%\" x=\"0\" y=\"0px\"><defs/><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">their</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs/><svg width=\"100%\" x=\"0\" y=\"0px\"><defs/><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">PRP$</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\"/></svg><line stroke=\"black\" x1=\"50%\" x2=\"91.784%\" y1=\"20px\" y2=\"48px\"/><svg width=\"5.16432%\" x=\"93.4272%\" y=\"48px\"><defs/><svg width=\"100%\" x=\"0\" y=\"0px\"><defs/><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">character</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs/><svg width=\"100%\" x=\"0\" y=\"0px\"><defs/><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\"/></svg><line stroke=\"black\" x1=\"50%\" x2=\"96.0094%\" y1=\"20px\" y2=\"48px\"/><svg width=\"1.40845%\" x=\"98.5915%\" y=\"48px\"><defs/><svg width=\"100%\" x=\"0\" y=\"0px\"><defs/><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">.</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs/><svg width=\"100%\" x=\"0\" y=\"0px\"><defs/><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">.</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\"/></svg><line stroke=\"black\" x1=\"50%\" x2=\"99.2958%\" y1=\"20px\" y2=\"48px\"/></svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def to_svg(tree):\n",
    "    return tree._repr_svg_()\n",
    "\n",
    "display(SVG(to_svg(entities)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea67148f",
   "metadata": {},
   "source": [
    "## Ein n-gram-Sprachmodell mit NLTK\n",
    "\n",
    "Mithilfe des NLTK können wir ebenfalls ein n-gram-Sprachmodell aufbauen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4f527b3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "although he neither saw nor heard — to feel — although he neither saw nor heard — to feel — although he neither saw nor heard — to feel the presence of my confidence , I found the eye always closed ; and so by degrees — very , very patiently ,\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "reader = open(\"data/poe-the-tell-tale-heart.txt\")\n",
    "story = reader.read()\n",
    "\n",
    "tokens = word_tokenize(story)\n",
    "\n",
    "# Build a trigram model \n",
    "model = defaultdict(list)\n",
    "for i in range(len(tokens)-2):\n",
    "    key = (tokens[i], tokens[i+1])\n",
    "    next_word = tokens[i+2]\n",
    "    model[key].append(next_word)\n",
    "\n",
    "# Generate text\n",
    "seed = random.choice(list(model.keys()))\n",
    "output = [seed[0], seed[1]]\n",
    "\n",
    "# generate 50 words\n",
    "for _ in range(50):  \n",
    "    next_word = random.choice(model.get((output[-2], output[-1]), [random.choice(tokens)]))\n",
    "    output.append(next_word)\n",
    "\n",
    "print(\" \".join(output))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e19fa06",
   "metadata": {},
   "source": [
    "Das ist noch nicht wirklich besser wie in unserem Ansatz im vorherigen Notebook. Wir können es ein wenig smarter machen, da unser NLTK-basierter Tokenizer bereits klassifizieren kann, ob ein Token ein Subjekt, ein Verb, ein Adjektiv, ein Objekt, ein Satzzeichen oder etwas anderes ist.\n",
    "\n",
    "Fügen wir diese Information zu unserem Modell hinzu, können wir mit einem 4-gram-Modell Texte generieren, die schon besser sind, und (teilweise) sogar Sinn ergeben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8ec5cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from collections import defaultdict, Counter\n",
    "import random\n",
    "\n",
    "reader = open(\"data/poe-the-tell-tale-heart.txt\")\n",
    "story = reader.read()\n",
    "\n",
    "paras = story.split('\\n\\n')\n",
    "\n",
    "tokens = []\n",
    "tagged = []\n",
    "\n",
    "for para in paras:\n",
    "    para_tokens = word_tokenize(para)\n",
    "    tagged += pos_tag(para_tokens)  # returns list of (word, POS)\n",
    "    tokens += para_tokens\n",
    "\n",
    "# --- Build POS-aware 4-gram model + 2-gram fallback ---\n",
    "model_4gram = defaultdict(list)\n",
    "model_3gram = defaultdict(list)\n",
    "model_2gram = defaultdict(list)\n",
    "starts = []  # valid sentence starters (avoid punctuation)\n",
    "\n",
    "punct = {\".\", \"!\", \"?\"}\n",
    "\n",
    "for i in range(len(tagged) - 3):\n",
    "\n",
    "    (w1, p1) = tagged[i]\n",
    "    (w2, p2) = tagged[i + 1]\n",
    "    (w3, p3) = tagged[i + 2]\n",
    "    (w4, p4) = tagged[i + 3]\n",
    "\n",
    "    key_4gram = (w1, p1, w2, p2, w3, p3)\n",
    "    model_4gram[key_4gram].append((w4, p4))\n",
    "\n",
    "    # Also build 2-gram fallback\n",
    "    key_3gram = (w1, p1, w2, p2)\n",
    "    model_3gram[key_3gram].append((w3, p3))\n",
    "    key_3gram = (w2, p2, w3, p3)\n",
    "    model_3gram[key_3gram].append((w4, p4))\n",
    "    \n",
    "    key_2gram = (w1, p1)\n",
    "    model_2gram[key_2gram].append((w2, p2))\n",
    "    key_2gram = (w2, p2)\n",
    "    model_2gram[key_2gram].append((w3, p3))\n",
    "    key_2gram = (w3, p3)\n",
    "    model_2gram[key_2gram].append((w4, p4))\n",
    "\n",
    "    # build list of good sentence starters\n",
    "    if w1[0].isupper() and w1 not in punct:\n",
    "        starts.append(((w1,p1), (w2, p2), (w3, p3)))\n",
    "\n",
    "# --- Text generation ---\n",
    "def generate(max_words=100, temperature=1.0):\n",
    "\n",
    "    # pick a clean sentence start\n",
    "    w1, w2, w3 = random.choice(starts)\n",
    "    output = [w1, w2, w3]\n",
    "\n",
    "    for _ in range(max_words):\n",
    "        last1, last2, last3 = output[-3], output[-2], output[-1]\n",
    "\n",
    "        key_4gram = (last1[0], last1[1], last2[0], last2[1], last3[0], last3[1])\n",
    "        key_3gram = (last2[0], last2[1], last3[0], last3[1])\n",
    "        key_2gram = (last3[0], last3[1])\n",
    "\n",
    "        # Try 4-gram first, fallback to trigram/bigram\n",
    "        options = model_4gram.get(key_4gram) or []\n",
    "        if len(options) <= 1:\n",
    "            options += options + options \n",
    "            options += model_3gram.get(key_3gram) or []\n",
    "        if len(options) <= 1:\n",
    "            options += options + options\n",
    "            options += model_2gram.get(key_2gram) or []\n",
    "        if len(options) == 0:\n",
    "            output.append((\".\", \".\"))\n",
    "            break\n",
    "\n",
    "        # Count occurrences of each word\n",
    "        word_counts = Counter(options)\n",
    "        words = list(word_counts.keys())\n",
    "        counts = np.array([word_counts[w] for w in words], dtype=float)\n",
    "\n",
    "        # Apply temperature scaling\n",
    "        counts = np.power(counts, 1.0 / temperature)\n",
    "        probs = counts / counts.sum()\n",
    "\n",
    "        next_word_idx = np.random.choice(range(len(words)), p=probs)\n",
    "        next_word = words[int(next_word_idx)]\n",
    "        # ---- RULES ----\n",
    "\n",
    "        # avoid punctuation at sentence start\n",
    "        if output[-1][0] in punct and next_word[0] in punct:\n",
    "            continue\n",
    "\n",
    "        output.append(next_word)\n",
    "\n",
    "        # force sentence break logic\n",
    "        if next_word[0] in punct:\n",
    "            break\n",
    "\n",
    "    sentence = \"\"\n",
    "    more_punct = {',', ':', ';'}\n",
    "    for token in output:\n",
    "        if not token[0] in punct and not token[0] in more_punct:\n",
    "            sentence += \" \"\n",
    "        sentence += token[0]\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "7f0c3e53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " And now have I not told you that what you mistake for madness is but over acuteness of the men — but the noise steadily increased.\n"
     ]
    }
   ],
   "source": [
    "print(generate(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52c1fb2",
   "metadata": {},
   "source": [
    "Wir generieren Sätze, die (meist) aussehen wie grammatikalisch korrektes Englisch :)\n",
    "\n",
    "Wir nutzen für die Ermittlung des Folge-Wortes einen Kontext von 3 Wörtern und haben ihm programmatisch beigebracht, dass Satzzeichen wie `.`, `?` oder `!` den Satz beenden. \n",
    "\n",
    "Eine Garantie, dass der Satz auch Sinn ergibt, gibt es hier nicht.\n",
    "\n",
    "Ein weiteres Problem ist, dass die Varianz nicht so gut ist. Es kommt vor, dass der Algorithmus Sätze 1 zu 1 aus dem Text übernimmt. \n",
    "\n",
    "Um die Varianz etwas zu vergrößern, wird, wenn es nur ein mögliches Folgewort gibt, ein verkleinerter Kontext verwendet. Das erhöht den Bullshit-Faktor jedoch nicht unwesentlich, daher gewichten wir die Folgewörter mit dem größerem Kontext etwas höher.\n",
    "\n",
    "Können wir unser Modell noch kreativer machen?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23872cde",
   "metadata": {},
   "source": [
    "## Die \"Temperatur\" eines Sprachmodells\n",
    "\n",
    "In Sprachmodellen beeinflusst die Temperatur eines Modells die Satzvervollständigung. Dieser Parameter steuert, wie vorhersehbar das Modell ist.\n",
    "\n",
    "Niedrige Werte führen zu vorhersehbaren bis deterministischen Antworten. Höhere Were erhöhen die Kreativität, aber damit auch das Risiko für Halluzinationen.\n",
    "\n",
    "Nachfolgend probieren wir unsere Generator-Funktion einmal mit hoher Temperatur (kreativ) und einmal mit niedriger Temperatur aus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5dcda887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " And every night, about midnight, when the day broke, I went to work!\n"
     ]
    }
   ],
   "source": [
    "print(generate(max_words=100, temperature=2000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "74afc67a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I am. And now a new anxiety seized me — the sound would be heard by a neighbor during the night; suspicion of foul play had been aroused; information had been lodged at the police office, and I fancied a ringing in my ears: but still they sat and still chatted.\n"
     ]
    }
   ],
   "source": [
    "print(generate(max_words=100, temperature=0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a3abc0",
   "metadata": {},
   "source": [
    "Mit dem Parameter 0.1 ist das Sprachmodell besonders \"konservativ\" gewichtet. Die Wahrscheinlichkeit ist höher, dass der generierte Satz Sinn ergibt. Allerdings ist die Varianz geringer und das Modell ist deterministischer. Große Teile zweiten Satzes wurden 1 zu 1 aus der Kurzgeschichte übernommen (z.B. \"And now a new anxiety seized me — the sound would be heard by \n",
    "a neighbor\").\n",
    "\n",
    "Eine hohe Temperatur sorgt für kreativere Ergebnisse und hält sich weniger fest an die Worthäufigkeiten. Es kann aber damit auch sein, dass der generierte Satz auch weniger Sinn ergibt (wobei um Mitternacht zu arbeiten vielleicht schon Sinn ergibt, wenn man Vampir, Programmierer oder beides ist). "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
